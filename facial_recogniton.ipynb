{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adf0a3f",
   "metadata": {},
   "source": [
    "# Facial Recognition Pipeline\n",
    "\n",
    "This project implements a facial recognition system that identifies faces by converting them into **128-dimensional vector embeddings**. Faces belonging to the same person should produce embeddings that are close together, while embeddings from different people should be farther apart. A simple classifier (like **KNN** or **SVM**) then maps these embeddings to identities.\n",
    "\n",
    "![Facial Recognition Pipeline](./examples/fast-five-1200-1200-675-675-crop-000000.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11579683",
   "metadata": {},
   "source": [
    "First, we need to install the required libraries. You can do this by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install face_recognition opencv-python scikit-learn imutils icrawler dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15204dbe",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this facial recognition project, we'll create a custom dataset of **Fast & Furious** actors. The dataset will include images of 10 main characters from the franchise:\n",
    "\n",
    "- **Vin Diesel** (Dominic Toretto)\n",
    "- **Paul Walker** (Brian O'Conner) \n",
    "- **Dwayne Johnson** (Luke Hobbs)\n",
    "- **Michelle Rodriguez** (Letty Ortiz)\n",
    "- **Tyrese Gibson** (Roman Pearce)\n",
    "- **Ludacris** (Tej Parker)\n",
    "- **Jordana Brewster** (Mia Toretto)\n",
    "- **Gal Gadot** (Gisele Yashar)\n",
    "- **Sung Kang** (Han Seoul-Oh)\n",
    "- **Jason Statham** (Deckard Shaw)\n",
    "\n",
    "### Dataset Structure\n",
    "```\n",
    "dataset/\n",
    "â”œâ”€â”€ Vin_Diesel/\n",
    "â”‚   â”œâ”€â”€ 000001.jpg\n",
    "â”‚   â”œâ”€â”€ 000002.jpg\n",
    "â”‚   â””â”€â”€ ... (30 images)\n",
    "â”œâ”€â”€ Paul_Walker/\n",
    "â”‚   â”œâ”€â”€ 000001.jpg\n",
    "â”‚   â””â”€â”€ ... (30 images)\n",
    "â””â”€â”€ ... (other actors)\n",
    "```\n",
    "\n",
    "### Data Collection\n",
    "We'll automatically download **30 images per actor** from Google Images using the `icrawler` library. This provides us with a diverse set of facial images for training our recognition model. The images will be stored in separate folders for each actor, making it easy to extract labels during the encoding process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6469c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import os\n",
    "\n",
    "actors = [\n",
    "    \"Tyrese Gibson\",\n",
    "]\n",
    "\n",
    "base_dir = \"dataset\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "def download_images(actor_name, max_images=30):\n",
    "    \"\"\"Download images of the actor from Google.\"\"\"\n",
    "    actor_dir = os.path.join(base_dir, actor_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(actor_dir, exist_ok=True)\n",
    "    \n",
    "    google_crawler = GoogleImageCrawler(storage={\"root_dir\": actor_dir})\n",
    "    google_crawler.crawl(keyword=actor_name + \" photoshoot\", max_num=max_images)\n",
    "    print(f\"âœ… Downloaded images for {actor_name}\")\n",
    "\n",
    "# Download images for all actors\n",
    "for actor in actors:\n",
    "    download_images(actor, max_images=30)\n",
    "\n",
    "print(\"ðŸŽ¯ Dataset ready in 'dataset/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a20fcf",
   "metadata": {},
   "source": [
    "## ðŸš€ Overview of the Pipeline\n",
    "\n",
    "The system is divided into 4 main stages:\n",
    "\n",
    "### 1. Detecting Faces (HOG-based Detection)\n",
    "\n",
    "The first step is locating faces in images using **Histogram of Oriented Gradients (HOG)**. Itâ€™s an efficient algorithm that works well for frontal faces.\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "- **1.1 Grayscale conversion:**  \n",
    "  Each image is first converted to grayscale to reduce computational complexity.\n",
    "\n",
    "- **1.2 Compute pixel gradients:**  \n",
    "  For every pixel, we calculate how dark it is compared to its immediate neighbors. This gives us a gradient vector showing the direction of intensity change.\n",
    "\n",
    "- **1.3 Aggregate gradient directions:**  \n",
    "  Since computing this for every pixel is too detailed, we divide the image into small cells (typically 16Ã—16 pixels). For each cell, we count how many gradients point in each major direction (e.g., 0Â°, 45Â°, 90Â°, etc.) and summarize the cell with a dominant direction.\n",
    "\n",
    "- **1.4 Compare with face templates:**  \n",
    "  The final HOG representation is compared with a known HOG pattern for a face to determine whether a face is present and where it is.\n",
    "```python\n",
    "import face_recognition\n",
    "\n",
    "image = face_recognition.load_image_file(\"example.jpg\")\n",
    "face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "\n",
    "```\n",
    "This gives us bounding boxes for the detected faces.\n",
    "\n",
    "![HOG Detection Example](./assets/face_detection.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Aligning Faces (Facial Landmark Projection)\n",
    "\n",
    "Once we detect a face, we want to align it so that facial features (like eyes and mouth) are in consistent positions across all images. This is important for the encoder to generate meaningful embeddings.\n",
    "\n",
    "We do this by finding **68 facial landmarks** for each face and warping the image accordingly.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "* A pre-trained model predicts the coordinates of 68 key facial features.\n",
    "* We use these points to **rotate, scale, and warp** the face so it is centered and normalized in the frame.\n",
    "\n",
    "#### Landmark Detection Details:\n",
    "\n",
    "Landmark detection models are typically based on **regression trees** or **deep CNNs**. One well-known implementation is Dlibâ€™s shape predictor, which was trained on labeled facial images to output consistent landmark coordinates. More advanced methods like **OpenFace** or **MediaPipe Face Mesh** use neural networks for more robust landmark estimation.\n",
    "\n",
    "```python\n",
    "import dlib\n",
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "image_bgr = cv2.imread(\"./examples/fast-five-1200-1200-675-675-crop-000000.jpg\")\n",
    "gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "face_locations = face_recognition.face_locations(image_rgb, model=\"hog\")\n",
    "\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    face_rect = dlib.rectangle(left, top, right, bottom)\n",
    "    \n",
    "    landmarks = predictor(gray, face_rect)\n",
    "```\n",
    "![Facial Landmark Example](./assets/landmarks_result.jpg)\n",
    "---\n",
    "\n",
    "### 3. Encoding Faces (128-Dimensional Embedding)\n",
    "\n",
    "After alignment, each face is passed through a **deep convolutional neural network** to extract a 128-dimensional embedding vector.\n",
    "\n",
    "This vector is trained to capture the identity of the person in such a way that:\n",
    "\n",
    "* Embeddings of the **same person** have a small Euclidean distance.\n",
    "* Embeddings of **different people** are far apart.\n",
    "\n",
    "We use a pre-trained encoder (like the one provided by `face_recognition`, based on dlib and similar to OpenFace) to handle this step.\n",
    "\n",
    "```python\n",
    "face_encodings = face_recognition.face_encodings(image, known_face_locations=face_locations)\n",
    "```\n",
    "\n",
    "Each face encoding is just a NumPy array of 128 float values.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Classifying Faces (KNN / SVM)\n",
    "\n",
    "At this point, face recognition becomes a classification problem:\n",
    "\n",
    "* For each person in your dataset, compute and store their embeddings.\n",
    "* When a new image comes in, generate its embedding and **compare it** to the stored ones.\n",
    "\n",
    "We can use:\n",
    "\n",
    "* **K-Nearest Neighbors (KNN):**\n",
    "  A simple and effective method. Given a new embedding, find the k closest embeddings in the dataset and vote.\n",
    "\n",
    "* **Support Vector Machine (SVM):**\n",
    "  Useful when you want a trained decision boundary between classes.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(known_embeddings, known_labels)\n",
    "\n",
    "prediction = knn.predict([new_embedding])\n",
    "```\n",
    "![Classification Example](./assets/classification.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379b0fe",
   "metadata": {},
   "source": [
    "Let's implement the pipeline using out dataset of Fast & Furious actors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2720ec1e",
   "metadata": {},
   "source": [
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34885ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import cv2\n",
    "import face_recognition\n",
    "import argparse\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b787eb",
   "metadata": {},
   "source": [
    "Define our dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce0d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./dataset\"\n",
    "encoding_path = \"./encodings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322dfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(data_path))\n",
    "knownEncodings = []\n",
    "knownNames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6025b5c",
   "metadata": {},
   "source": [
    "Encode the faces using the `face_recognition` library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bcb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "\t# extract the person name from the image path\n",
    "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "\t\tlen(imagePaths)))\n",
    "\tname = imagePath.split(os.path.sep)[-2]\n",
    "\tprint(name)\n",
    "\t# load the input image and convert it from BGR (OpenCV ordering)\n",
    "\t# to dlib ordering (RGB)\n",
    "\timage = cv2.imread(imagePath)\n",
    "\trgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tboxes = face_recognition.face_locations(rgb,\n",
    "\t\tmodel=\"hog\")\n",
    "\t# compute the facial embedding for the face\n",
    "\tencodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\t# loop over the encodings\n",
    "\tfor encoding in encodings:\n",
    "\t\t# add each encoding + name to our set of known names and\n",
    "\t\t# encodings\n",
    "\t\tknownEncodings.append(encoding)\n",
    "\t\tknownNames.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6c527",
   "metadata": {},
   "source": [
    "Save our embeddings and labels for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8ed582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Face encodings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "with open(encoding_path+\"/encoding.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(data))\n",
    "print(\"[INFO] Face encodings saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849e49f",
   "metadata": {},
   "source": [
    "Now that we've encoded all the faces in our dataset, let's see how the recognition process works.\n",
    "\n",
    "#### 1. **Load Saved Encodings**\n",
    "```python\n",
    "data = pickle.loads(open(encoding_path+\"/encoding.pkl\", \"rb\").read())\n",
    "```\n",
    "We load our previously saved face encodings and corresponding names from the pickle file.\n",
    "\n",
    "#### 2. **Process Each Test Image**\n",
    "For every image in our `./examples/` folder:\n",
    "- **Load the image** using OpenCV\n",
    "- **Convert color space** from BGR to RGB (required by face_recognition library)\n",
    "- **Detect faces** using HOG-based detection\n",
    "- **Generate encodings** for detected faces\n",
    "\n",
    "#### 3. **Face Matching Algorithm**\n",
    "For each detected face encoding:\n",
    "\n",
    "**Step 3.1: Compare with Known Faces**\n",
    "```python\n",
    "matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "```\n",
    "This compares the current face encoding with all encodings in our dataset, returning a boolean list. The function calculates the Euclidean distance between the new face encoding and each of the known encodings in your list.\n",
    "\n",
    "**Step 3.2: Find Best Match Using Voting**\n",
    "```python\n",
    "if True in matches:\n",
    "    matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "    counts = {}\n",
    "    for i in matchedIdxs:\n",
    "        name = data[\"names\"][i]\n",
    "        counts[name] = counts.get(name, 0) + 1\n",
    "    name = max(counts, key=counts.get)\n",
    "```\n",
    "\n",
    "This implements a **voting mechanism**:\n",
    "- Collect all matching face indices\n",
    "- Count votes for each person name\n",
    "- Select the name with the most votes (handles multiple encodings per person)\n",
    "\n",
    "\n",
    "This testing phase validates that our face recognition pipeline can successfully identify the Fast & Furious actors from new images not used during the encoding phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799fc96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names detected in ./examples/fast-five-1200-1200-675-675-crop-000000.jpg: ['Jordana_Brewster', 'Vin_Diesel', 'Paul_Walker', 'Gal_Gadot', 'Dwayne_Johnson']\n"
     ]
    }
   ],
   "source": [
    "data = pickle.loads(open(encoding_path+\"/encoding.pkl\", \"rb\").read())\n",
    "for imagePath in list(paths.list_images(\"./examples\")):\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    boxes = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    \n",
    "    names = []\n",
    "    for encoding in encodings:\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        if True in matches:\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "            name = max(counts, key=counts.get)\n",
    "        \n",
    "        names.append(name)\n",
    "    print(f\"Names detected in {imagePath}: {names}\")\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.imwrite(f\"./output/{os.path.basename(imagePath)}\", image)\n",
    "    cv2.waitKey(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c471f",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Real-Time Video Face Recognition\n",
    "\n",
    "Now let's apply our trained face recognition system to process video files. This demonstrates how the pipeline performs on moving images with multiple faces appearing and disappearing throughout the video.\n",
    "\n",
    "### Video Processing Features:\n",
    "\n",
    "- **Frame-by-frame analysis**: Process each video frame to detect and identify faces\n",
    "- **Optimized performance**: Skip frames to balance accuracy with processing speed\n",
    "- **Real-time display**: Show recognition results as the video plays\n",
    "- **Output generation**: Save processed video with face annotations\n",
    "- **Smooth tracking**: Use previous frame results for skipped frames to maintain consistency\n",
    "\n",
    "### Video Demo:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/fast5.gif\" alt=\"Video Demo\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d495e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing video: .video/fast5-lq.mp4...\n",
      "[INFO] Video properties: FPS=0, Width=0, Height=0\n",
      "[INFO] cleaning up...\n",
      "[INFO] Output video saved to: ./output/video/output_fast5.mp4\n"
     ]
    }
   ],
   "source": [
    "data = pickle.loads(open(encoding_path+\"/encoding.pkl\", \"rb\").read())\n",
    "\n",
    "video_path = \".video/fast5-lq.mp4\" # Path to your video file\n",
    "output_path = \"./output/video/output_fast5.mp4\" # Path to save the output video\n",
    "vs = cv2.VideoCapture(video_path)\n",
    "print(f\"[INFO] processing video: {video_path}...\")\n",
    "\n",
    "# Get video properties for output video\n",
    "fps = int(vs.get(cv2.CAP_PROP_FPS))\n",
    "width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"[INFO] Video properties: FPS={fps}, Width={width}, Height={height}\")\n",
    "\n",
    "\n",
    "\n",
    "# Frame skipping configuration\n",
    "frame_skip = 3  # Process every 6th frame (adjust this value as needed)\n",
    "frame_count = 0\n",
    "output_fps = 20\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, output_fps, (width, height))\n",
    "# Variables to store last detection results\n",
    "last_boxes = []\n",
    "last_names = []\n",
    "\n",
    "while True:\n",
    "    # Grab the next frame from the video stream\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # If the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Only process every nth frame for face detection\n",
    "    if frame_count % frame_skip == 0:\n",
    "        # --- YOUR LOGIC (applied to 'frame' instead of 'image') ---\n",
    "        # Convert the input frame from BGR to RGB\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect face locations and compute encodings\n",
    "        boxes = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        names = []\n",
    "\n",
    "        # Loop over the facial embeddings\n",
    "        for encoding in encodings:\n",
    "            matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            if True in matches:\n",
    "                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "                counts = {}\n",
    "                for i in matchedIdxs:\n",
    "                    name = data[\"names\"][i]\n",
    "                    counts[name] = counts.get(name, 0) + 1\n",
    "                name = max(counts, key=counts.get)\n",
    "            \n",
    "            names.append(name)\n",
    "        \n",
    "        # Store the results for use in skipped frames\n",
    "        last_boxes = boxes\n",
    "        last_names = names\n",
    "    else:\n",
    "        # Use the last detection results for skipped frames\n",
    "        boxes = last_boxes\n",
    "        names = last_names\n",
    "\n",
    "    # Loop over the recognized faces to draw boxes and names\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # If the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# --- NEW: Clean up ---\n",
    "print(\"[INFO] cleaning up...\")\n",
    "vs.release()\n",
    "out.release()  # Release the video writer\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"[INFO] Output video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11cacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
